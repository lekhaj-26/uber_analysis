# -*- coding: utf-8 -*-
"""Personal Project(Uber Analysis).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hVxQ-vAvC7A20T3RvXegVLw83MnZ3AFB
"""


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import folium

data=pd.read_csv('Uber-Jan-Feb-FOIL.csv')

data.head()

data.shape

data.info()

data.describe()

data.isnull().sum()

#Conversion of date/time column to datetime object
data['date']=pd.to_datetime(data['date'])

# Extract time components from the 'date' column
data['Hour'] = data['date'].dt.hour
data['Day'] = data['date'].dt.day
data['Month'] = data['date'].dt.month
data['DayOfWeek'] = data['date'].dt.dayofweek  # 0=Monday, 6=Sunday

#Removing of duplicates
data = data.dropna()
data = data.drop_duplicates()
data = data.reset_index(drop=True)
print(data.info())

"""**EDA(Temporal Analysis)**"""

#Pickups per hour
hourly = data['Hour'].value_counts().sort_index()

plt.figure(figsize=(10, 5))
sns.barplot(x=hourly.index, y=hourly.values, hue=hourly.index, palette='viridis', legend=True) # Changed 'True' to True
plt.title("Pickups per Hour")
plt.xlabel("Hour of the Day")
plt.ylabel("Number of Pickups")
plt.xticks(range(0, 24))
plt.show()

#Pickups per day
daily = data['Day'].value_counts().sort_index()

plt.figure(figsize=(10, 5))
sns.barplot(x=daily.index, y=daily.values, hue=daily.index, palette='rocket', legend=True) # Changed 'True' to True
plt.title("Pickups per Day of the Month")
plt.xlabel("Day")
plt.ylabel("Number of Pickups")
plt.show()


#Pickups per weekday
# 0 = Monday, 6 = Sunday
weekday = data['DayOfWeek'].value_counts().sort_index()
days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']

plt.figure(figsize=(10, 5))
sns.barplot(x=days, y=weekday.values, hue=days, palette='coolwarm', legend=True) # Changed 'True' to True
plt.title("Pickups per Day of the Week")
plt.xlabel("Day of the Week")
plt.ylabel("Number of Pickups")
plt.show()

# Create a pivot table with day and hour
heatmap_data = data.groupby(['Day', 'Hour']).size().unstack()

plt.figure(figsize=(12, 6))
sns.heatmap(heatmap_data, cmap="YlGnBu")
plt.title("Heatmap of Pickups: Day vs Hour")
plt.xlabel("Hour of the Day")
plt.ylabel("Day of the Month")
plt.show()

monthly = data['Month'].value_counts().sort_index()
month = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']

monthly_df = pd.DataFrame({
    'Month': [month[m - 1] for m in monthly.index],
    'Count': monthly.values
})

plt.figure(figsize=(10, 5))
sns.barplot(x='Month', y='Count', hue='Month', data=monthly_df, palette='crest', legend=False)
plt.title("Monthly Pickup Trend")
plt.xlabel("Month")
plt.ylabel("Number of Pickups")
plt.show()

"""**EDA(Spatial Analysis)**"""

data.columns

import folium
from folium.plugins import HeatMap
import pandas as pd

data = pd.read_csv("uber-raw-data-sep14.csv")
# Use the correct column name 'date' as it exists in the CSV file
data['Date/Time'] = pd.to_datetime(data['Date/Time']) # Changed 'Date/Time' to 'date'
data.rename(columns={'Lat': 'Latitude', 'Lon': 'Longitude'}, inplace=True)
print(data[['Latitude', 'Longitude']].head())

# Remove rows with missing or invalid coordinates
data = data[(data['Latitude'].between(40.5, 41.0)) & (data['Longitude'].between(-74.3, -73.6))]

#plotting pickups on a folium map
# Center on NYC
nyc_map = folium.Map(location=[40.7128, -74.0060], zoom_start=12)

# Add sample markers
for index, row in data.head(100).iterrows():
    folium.CircleMarker(
        location=[row['Latitude'], row['Longitude']],
        radius=2,
        color='blue',
        fill=True,
        fill_opacity=0.5
    ).add_to(nyc_map)
nyc_map.save("nyc_pickup_map.html")
nyc_map

# Create a heatmap with all pickup points (limit to avoid memory issues)
pickup_locations = data[['Latitude', 'Longitude']].values.tolist()

heat_map = folium.Map(location=[40.7128, -74.0060], zoom_start=12)

HeatMap(pickup_locations[:10000], radius=7, blur=5).add_to(heat_map)
nyc_map.save("nyc_heatmap.html")
heat_map

b_data = data[(data['Latitude'] > 40.65) & (data['Latitude'] < 40.7) &
                 (data['Longitude'] > -74.0) & (data['Longitude'] < -73.85)]
nyc_map.save("b_data")
heat_map


"""**EDA(Base Analsis)**"""

#Count pickups per base

base_counts = data['Base'].value_counts()

# Plot the counts
plt.figure(figsize=(10, 5))
sns.barplot(x=base_counts.index, y=base_counts.values, hue=base_counts, palette='magma', legend=False)
plt.title("Total Pickups by Base")
plt.xlabel("Base ID")
plt.ylabel("Number of Pickups")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


#Time trends for each base

data['Date'] = data['Date/Time'].dt.date
# Groupby Date and Base, count number of pickups
daily_base = data.groupby(['Date', 'Base']).size().reset_index(name='Pickups')


#Plot time trends for top 3 bases

# Find top 3 bases
top_bases = base_counts.head(3).index

# Filter data for top bases
filtered = daily_base[daily_base['Base'].isin(top_bases)]

plt.figure(figsize=(12, 6))
for base in top_bases:
    subset = filtered[filtered['Base'] == base]
    plt.plot(subset['Date'], subset['Pickups'], label=base)

plt.title("Pickup Trends Over Time by Base")
plt.xlabel("Date")
plt.ylabel("Number of Pickups")
plt.xticks(rotation=45)
plt.legend(title="Base")
plt.tight_layout()
plt.show()

"""### **Visualization**"""

#1. Line chart: Hourly Pickups

# Extract hour
data['Hour'] = data['Date/Time'].dt.hour

# Count pickups by hour
hourly = data['Hour'].value_counts().sort_index()

# Plot
plt.figure(figsize=(10, 5))
sns.lineplot(x=hourly.index, y=hourly.values, marker='o')
plt.title("Hourly Uber Pickups")
plt.xlabel("Hour of the Day (0-23)")
plt.ylabel("Number of Pickups")
plt.grid(True)
plt.show()


#2. Bar Plot: Pickups per day of week

# Extract day of the week
data['Weekday'] = data['Date/Time'].dt.day_name()

# Count pickups per weekday
weekday_counts = data['Weekday'].value_counts().reindex([
    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'
])

# Plot
plt.figure(figsize=(10, 5))
sns.barplot(x=weekday_counts.index, y=weekday_counts.values, hue=weekday_counts, palette='viridis')
plt.title("Uber Pickups by Day of the Week")
plt.xlabel("Day of the Week")
plt.ylabel("Number of Pickups")
plt.xticks(rotation=45)
plt.show()


#3. Heatmap: Hour vs Day

# Extract hour and day
data['Day'] = data['Date/Time'].dt.day

# Create pivot table
heatmap_data = data.groupby(['Day', 'Hour']).size().unstack()

# Plot
plt.figure(figsize=(12, 6))
sns.heatmap(heatmap_data, cmap="YlGnBu")
plt.title("Heatmap of Pickups: Day vs Hour")
plt.xlabel("Hour of the Day")
plt.ylabel("Day of the Month")
plt.show()


#4. Geographic Heatmap of Pickup Density

import folium
from folium.plugins import HeatMap

# Convert coordinates to list format
pickup_locations = data[['Latitude', 'Longitude']].dropna().values.tolist()

# Create map centered on NYC
heat_map = folium.Map(location=[40.7128, -74.0060], zoom_start=12)

# Adding of heatmap layer (limited to 10,000 points for performance)
HeatMap(pickup_locations[:10000], radius=7, blur=5).add_to(heat_map)
heat_map

"""### **Feature Engineering**"""

#1. Cluster Pickups Using K-Means

from sklearn.cluster import KMeans
import numpy as np
import matplotlib.pyplot as plt

# Take sample data for faster computation
coords = data[['Latitude', 'Longitude']].dropna().sample(10000, random_state=42)

# Apply K-Means clustering
kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(coords)

# Add cluster labels back to dataframe
coords['Cluster'] = kmeans.labels_

# Plot clusters
plt.figure(figsize=(10, 8))
sns.scatterplot(data=coords, x='Longitude', y='Latitude', hue='Cluster', palette='tab10', s=10)
plt.title("K-Means Clustering of Pickup Locations (5 Zones)")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.legend(title="Cluster")
plt.show()

#2. Applying Haverine Distance

import numpy as np

def haversine(lat1, lon1, lat2, lon2):
    R = 6371  # Earth compass in km

    phi1 = np.radians(lat1)
    phi2 = np.radians(lat2)
    d_phi = np.radians(lat2 - lat1)
    d_lambda = np.radians(lon2 - lon1)

    a = np.sin(d_phi/2)**2 + np.cos(phi1)*np.cos(phi2)*np.sin(d_lambda/2)**2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))

    return R * c  # Distance in km

# Times Square coordinates
ref_lat, ref_lon = 40.7580, -73.9855

# Apply haversine distance to all points
data['Distance_km'] = data.apply(lambda row: haversine(ref_lat, ref_lon, row['Latitude'], row['Longitude']), axis=1)

# Histogram of distances
plt.figure(figsize=(10, 5))
sns.histplot(data['Distance_km'], bins=50, kde=True)
plt.title("Distribution of Pickup Distances from Times Square")
plt.xlabel("Distance (km)")
plt.ylabel("Number of Pickups")
plt.show()

"""### **Predictive Modelling**"""

data['Date/Time'] = pd.to_datetime(data['Date/Time'])

# Extract features
data['Hour'] = data['Date/Time'].dt.hour
data['Day'] = data['Date/Time'].dt.day
data['Weekday'] = data['Date/Time'].dt.weekday  # 0=Monday
data['Month'] = data['Date/Time'].dt.month

# Round datetime to hourly buckets
data['Datetime_Hour'] = data['Date/Time'].dt.floor('h')

# Group data to get hourly pickup counts
grouped = data.groupby(['Datetime_Hour', 'Hour', 'Day', 'Weekday', 'Month']).size().reset_index(name='Pickup_Count')




from sklearn.model_selection import train_test_split

# Define features and target
X = grouped[['Hour', 'Day', 'Weekday', 'Month']]
y = grouped['Pickup_Count']

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)




#1. Linear Regression
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

print("Linear Regression RMSE:", mean_squared_error(y_test, y_pred_lr))


#2. Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print("Random Forest RMSE:", mean_squared_error(y_test, y_pred_rf))


#3.XGBoost Regressor
from xgboost import XGBRegressor

xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
xgb.fit(X_train, y_train)
y_pred_xgb = xgb.predict(X_test)

print("XGBoost RMSE:", mean_squared_error(y_test, y_pred_xgb))


#Visualizing Predictions
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(y_test.values[:100], label='Actual', marker='o')
plt.plot(y_pred_rf[:100], label='Predicted (RF)', marker='x')
plt.title("Predicted vs Actual Pickup Counts")
plt.xlabel("Sample Index")
plt.ylabel("Pickup Count")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""### **Insights**"""

#1. Estimating the peak pickup hours

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

data['Date/Time'] = pd.to_datetime(data['Date/Time'])
data['Hour'] = data['Date/Time'].dt.hour

# Plot pickups by hour
plt.figure(figsize=(12, 6))
sns.countplot(x='Hour', data=data, hue='Hour', palette='viridis')
plt.title('Uber Pickups by Hour of the Day')
plt.xlabel('Hour of Day (0-23)')
plt.ylabel('Number of Pickups')
plt.show()



#2. Estimating which weekdays have higher demand

data['Weekday'] = data['Date/Time'].dt.day_name()

plt.figure(figsize=(12, 6))
sns.countplot(x='Weekday', data=data,
              order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], hue='Weekday',
              palette='coolwarm')
plt.title('Uber Pickups by Day of the Week')
plt.xlabel('Day of the Week')
plt.ylabel('Number of Pickups')
plt.show()



#3. How can Uber better allocate drivers?

from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error

# Group by hour and weekday
data['Weekday_num'] = data['Date/Time'].dt.weekday
hourly_data = data.groupby(['Hour', 'Weekday_num']).size().reset_index(name='Pickups')

X = hourly_data[['Hour', 'Weekday_num']]
y = hourly_data['Pickups']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = XGBRegressor(n_estimators=100, learning_rate=0.1)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
rmse = mean_squared_error(y_test, y_pred)
print("RMSE of pickup prediction model:", rmse)

# Commented out IPython magic to ensure Python compatibility.
#4. Any specific zones with higher concentration of pickups

import folium
from folium.plugins import HeatMap
import pandas as pd
data = pd.read_csv("uber-raw-data-sep14.csv")
data['Date/Time'] = pd.to_datetime(data['Date/Time'])
data.rename(columns={'Lat': 'Latitude', 'Lon': 'Longitude'}, inplace=True)
data = data[(data['Latitude'].between(40.5, 41.0)) & (data['Longitude'].between(-74.3, -73.6))]
location_data = data[['Latitude', 'Longitude']].dropna()
base_map = folium.Map(location=[40.75, -73.97], zoom_start=11)
HeatMap(data=location_data.values.tolist(), radius=8).add_to(base_map)
base_map.save("nyc_heatmap.html")


